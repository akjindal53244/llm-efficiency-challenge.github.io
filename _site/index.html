<!DOCTYPE html>
<html lang=" en-US">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    
    <meta charset="UTF-8">

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>NeurIPS Large Language Model Efficiency Challenge:1 LLM + 1GPU + 1Day | NeurIPS 2023 Challenge</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="NeurIPS Large Language Model Efficiency Challenge:1 LLM + 1GPU + 1Day">
<meta property="og:locale" content="en_US">
<meta name="description" content="NeurIPS 2023 Challenge">
<meta property="og:description" content="NeurIPS 2023 Challenge">
<link rel="canonical" href="http://localhost:4000/">
<meta property="og:url" content="http://localhost:4000/">
<meta property="og:site_name" content="NeurIPS Large Language Model Efficiency Challenge:1 LLM + 1GPU + 1Day">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="NeurIPS Large Language Model Efficiency Challenge:1 LLM + 1GPU + 1Day">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"NeurIPS 2023 Challenge","headline":"NeurIPS Large Language Model Efficiency Challenge:1 LLM + 1GPU + 1Day","name":"NeurIPS Large Language Model Efficiency Challenge:1 LLM + 1GPU + 1Day","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&amp;display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <style type="text/css">
        /* Style the tab */
        .tab {
            overflow: hidden;
            border: 1px solid #ccc;
            background-color: #fdfdff;
        }

        /* Style the buttons that are used to open the tab content */
        .tab button {
            background-color: inherit;
            float: left;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 16px;
            transition: 0.3s;
        }

        /* Change background color of buttons on hover */
        .tab button:hover {
            background-color: #ddd;
        }

        /* Create an active/current tablink class */
        .tab button.active {
            background-color: #ccc;
        }

        /* Style the tab content */
        .tabcontent {
            display: none;
            padding: 6px 12px;
            border: 1px solid #ccc;
            border-top: none;
        }
    </style>


</head>

<body>
    <header class="page-header" role="banner">
        <h1 class="project-name">NeurIPS Large Language Model Efficiency Challenge:<br>1 LLM + 1GPU + 1Day</h1>
        <h2 class="project-tagline">NeurIPS 2023 Challenge</h2>
        
        <a href="index" class="btn">Home</a>
        
        <a href="challenge" class="btn">Challenge</a>
        
        <a href="rules" class="btn">Rules</a>
        
        <a href="dates" class="btn">Timeline</a>
        
        <a href="prizes" class="btn">Prizes</a>
        
        <a href="starter_kit" class="btn">Starter Kit</a>
        
        <a href="leaderboard" class="btn">Leaderboard</a>
        
        <a href="organizers" class="btn">Organizers</a>
        
        <a href="advisors" class="btn">Advisors</a>
        
        <a href="sponsors" class="btn">Sponsors</a>
        
        <a href="question" class="btn">Contact Us</a>
        

    </header>

    <main id="content" class="main-content" role="main">
        <p style="text-align: justify;">
Large Language Models (LLMs) trained on large corpora of texts have attracted significant attention in recent years with their ability to solve tasks with few supervised examples. These few-shot models have shown state-of-the-art success across NLP tasks (Entity Recognition), language translation, standardized exams (SAT, AP exams, Medical Knowledge), coding challenges (LeetCode), as well as in subjective domains such as chatbots. All of these domains involve bootstrapping a single LLM referred to as a foundation model with examples of specific knowledge from the associated task. The process of updating a model with limited domain-specific data is known as fine-tuning. However, the costs of accessing, fine-tuning and querying foundation models to perform new tasks are large. Given these costs, access to performant LLMs has been gated behind expensive and often proprietary hardware used to train models, making them inaccessible to those without substantial resources. </p>

<p><br>
Our goal is to democratize access to language models and address three major issues:</p>

<ol>
  <li>Lack of transparency around model training methods leads to a majority of models being not reproducible. Preprint. Under review.</li>
  <li>The absence of a standard benchmark to evaluate these models side-by-side.</li>
  <li>Insufficient access to dedicated hardware prevents widespread availability and usage of these models.</li>
</ol>

<p><br></p>

<p style="text-align: justify;">

Here we present a LLM efficiency challenge, to tackle these three challenges and democratize access to state of the art LLMs. Specifically, we introduce a challenge for the community to adapt a foundation model to specific tasks by fine-tuning on a <strong>single GPU</strong> of either 4090 or A100 within a <strong>24-hour</strong> (1-day) time frame, while maintaining high accuracy for these desired tasks. Beyond providing a suite of evaluation tasks, we will perform extensive analysis on each submission to study accuracy and computational performance tradeoffs at commodity hardware scales. The outcome of this competition is to distill the insights and lessons into a set of well documented steps and easy-to-follow tutorials. The ML community will have documentation on how to achieve the same performance as winning entries, which will serve as the starting point to help them build their own LLM solutions.

</p>


        <footer class="site-footer">
            <hr>
            <!-- 
            <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub
                    Pages</a>.</span> -->
        </footer>
    </main>

</body>

</html>
